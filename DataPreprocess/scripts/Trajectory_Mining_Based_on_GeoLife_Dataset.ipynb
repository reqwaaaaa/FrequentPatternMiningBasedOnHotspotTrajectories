{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T06:49:45.920615Z",
     "start_time": "2025-04-17T06:49:44.421997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 基础模块\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# 可视化配置\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['font.family'] = 'STHeiti'         # 中文支持（macOS）\n",
    "plt.rcParams['axes.unicode_minus'] = False      # 正负号支持\n",
    "%matplotlib inline\n",
    "\n",
    "# 轨迹聚类与建模\n",
    "from sklearn.cluster import DBSCAN\n",
    "from pyproj import Transformer\n",
    "\n",
    "# API调用（如POI增强）\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Neo4j 图数据库\n",
    "from py2neo import Graph, Node, Relationship  # 若报错先注释，等后面阶段再装\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# 路径配置\n",
    "base_dir = os.path.dirname(os.path.abspath(\"__file__\"))  # 当前脚本所在目录\n",
    "root_dir = os.path.abspath(os.path.join(base_dir, '..'))  # 项目根目录\n",
    "output_dir = os.path.join(root_dir, 'outputs')\n",
    "traj_path = os.path.join(output_dir, 'geolife_cleaned_traj.csv')"
   ],
   "id": "c1c197d2efc8e319",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-16T11:14:22.869455Z",
     "start_time": "2025-04-16T11:14:22.687798Z"
    }
   },
   "source": [
    "# Step 1：加载数据\n",
    "df = pd.read_csv(traj_path)\n",
    "df['t'] = pd.to_datetime(df['t'])\n",
    "\n",
    "# Step 2：空间聚类识别热点节点\n",
    "eps = 0.0006      # 空间阈值（近似50米）\n",
    "min_samples = 5\n",
    "db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "df['node'] = db.fit_predict(df[['x', 'y']])\n",
    "\n",
    "# 移除噪声节点\n",
    "df = df[df['node'] != -1].reset_index(drop=True)\n",
    "\n",
    "# Step 3：构建轨迹节点序列 & 时间序列\n",
    "traj_records = []\n",
    "\n",
    "for (uid, traj_id), group in df.groupby(['uid', 'traj_id']):\n",
    "    group = group.sort_values('t')\n",
    "    nodes = group['node'].tolist()\n",
    "    times = group['t'].tolist()\n",
    "\n",
    "    # 去除连续重复节点\n",
    "    clean_nodes = [nodes[0]]\n",
    "    clean_times = [times[0]]\n",
    "    for i in range(1, len(nodes)):\n",
    "        if nodes[i] != clean_nodes[-1]:\n",
    "            clean_nodes.append(nodes[i])\n",
    "            clean_times.append(times[i])\n",
    "\n",
    "    if len(clean_nodes) >= 2:\n",
    "        traj_records.append({\n",
    "            'uid': int(uid),\n",
    "            'traj_id': int(traj_id),\n",
    "            'start_time': clean_times[0].time(),\n",
    "            'end_time': clean_times[-1].time(),\n",
    "            'node_sequence': json.dumps(clean_nodes, ensure_ascii=False),\n",
    "            'time_sequence': json.dumps([t.strftime(\"%H:%M:%S\") for t in clean_times], ensure_ascii=False)\n",
    "        })\n",
    "\n",
    "print(f\"构建完成，共记录轨迹数：{len(traj_records)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "构建完成，共记录轨迹数：1181\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "7c1f7de43b27018b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T11:15:34.135038Z",
     "start_time": "2025-04-16T11:15:34.125557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "traj_meta_df = pd.DataFrame(traj_records)\n",
    "metadata_path = os.path.join(output_dir, 'traj_metadata.csv')\n",
    "traj_meta_df.to_csv(metadata_path, index=False)\n",
    "\n",
    "print(f\"轨迹元数据表已保存：{metadata_path}\")"
   ],
   "id": "95ae1913359dcf98",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "轨迹元数据表已保存：/Users/chenenqiang/Desktop/Undergraduate Life/Undergraduate Life/创新实验2025春/FrequentPatternMiningBasedOnHotspotTrajectories/DataPreprocess/outputs/traj_metadata.csv\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "8d3a4bcd06054be4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T11:19:21.894151Z",
     "start_time": "2025-04-16T11:19:21.846746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 路径配置\n",
    "base_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "root_dir = os.path.abspath(os.path.join(base_dir, '..'))\n",
    "output_dir = os.path.join(root_dir, 'outputs')\n",
    "meta_path = os.path.join(output_dir, 'traj_metadata.csv')\n",
    "\n",
    "# 加载轨迹元数据\n",
    "df = pd.read_csv(meta_path)\n",
    "df['node_sequence'] = df['node_sequence'].apply(json.loads)\n",
    "\n",
    "# 构建N度路径表：SN → SG集合（uid, traj_id）\n",
    "ndegree_path_table = defaultdict(set)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    uid = int(row['uid'])\n",
    "    traj_id = int(row['traj_id'])\n",
    "    nodes = row['node_sequence']\n",
    "    path_len = len(nodes)\n",
    "\n",
    "    for n in range(1, path_len):  # 枚举所有n阶路径（至少2个节点）\n",
    "        for i in range(path_len - n):\n",
    "            subpath = tuple(nodes[i:i + n + 1])  # 如 [1,2], [2,3,4]\n",
    "            ndegree_path_table[subpath].add((uid, traj_id))\n",
    "\n",
    "print(f\"N度路径构建完成，共有唯一路径：{len(ndegree_path_table)} 条\")\n",
    "\n",
    "# 输出为标准CSV结构：SN, SG, k\n",
    "ndegree_path_df = pd.DataFrame([\n",
    "    {\n",
    "        'SN': json.dumps(list(path), ensure_ascii=False),\n",
    "        'SG': json.dumps([list(pair) for pair in sorted(sg_set)], ensure_ascii=False),\n",
    "        'k': len(path)\n",
    "    }\n",
    "    for path, sg_set in ndegree_path_table.items()\n",
    "])\n",
    "\n",
    "output_file = os.path.join(output_dir, 'ndegree_path_table.csv')\n",
    "ndegree_path_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"N度路径表已保存至：{output_file}\")"
   ],
   "id": "10d8a58c68849b89",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N度路径构建完成，共有唯一路径：5970 条\n",
      "N度路径表已保存至：/Users/chenenqiang/Desktop/Undergraduate Life/Undergraduate Life/创新实验2025春/FrequentPatternMiningBasedOnHotspotTrajectories/DataPreprocess/outputs/ndegree_path_table.csv\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "c400eb03135b368b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T11:26:33.425553Z",
     "start_time": "2025-04-16T11:26:33.362764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 加载原始轨迹数据\n",
    "df = pd.read_csv(traj_path)\n",
    "\n",
    "# DBSCAN聚类参数（保持与之前完全一致）\n",
    "eps = 0.0006\n",
    "min_samples = 5\n",
    "db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "df['node'] = db.fit_predict(df[['x', 'y']])\n",
    "df = df[df['node'] != -1]  # 移除噪声\n",
    "\n",
    "# 计算每个 node_id 的坐标中心\n",
    "node_coords = df.groupby('node')[['x', 'y']].mean().reset_index()\n",
    "node_coords.columns = ['node_id', 'x', 'y']\n",
    "\n",
    "# 保存为 nodes.csv\n",
    "nodes_path = os.path.join(output_dir, 'nodes.csv')\n",
    "node_coords.to_csv(nodes_path, index=False)\n",
    "print(f\"节点中心文件已保存至：{nodes_path}\")"
   ],
   "id": "907dc52d8c5f5f6c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "节点中心文件已保存至：/Users/chenenqiang/Desktop/Undergraduate Life/Undergraduate Life/创新实验2025春/FrequentPatternMiningBasedOnHotspotTrajectories/DataPreprocess/outputs/nodes.csv\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T11:27:21.882155Z",
     "start_time": "2025-04-16T11:27:21.850336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 读取 traj_metadata\n",
    "meta_path = os.path.join(output_dir, 'traj_metadata.csv')\n",
    "meta_df = pd.read_csv(meta_path)\n",
    "meta_df['node_sequence'] = meta_df['node_sequence'].apply(json.loads)\n",
    "\n",
    "# 构建边频率与所属轨迹集合\n",
    "edge_freq = defaultdict(int)\n",
    "edge_trajs = defaultdict(set)\n",
    "\n",
    "for _, row in meta_df.iterrows():\n",
    "    uid, traj_id = int(row['uid']), int(row['traj_id'])\n",
    "    nodes = row['node_sequence']\n",
    "\n",
    "    for i in range(len(nodes) - 1):\n",
    "        edge = (nodes[i], nodes[i+1])\n",
    "        edge_freq[edge] += 1\n",
    "        edge_trajs[edge].add((uid, traj_id))\n",
    "\n",
    "# 构建输出表\n",
    "edges_df = pd.DataFrame([\n",
    "    {\n",
    "        'source': src,\n",
    "        'target': tgt,\n",
    "        'frequency': edge_freq[(src, tgt)],\n",
    "        'traj_ids': json.dumps(sorted([list(x) for x in edge_trajs[(src, tgt)]]), ensure_ascii=False)\n",
    "    }\n",
    "    for (src, tgt) in edge_freq\n",
    "])\n",
    "\n",
    "# 保存为 edges.csv\n",
    "edges_path = os.path.join(output_dir, 'edges.csv')\n",
    "edges_df.to_csv(edges_path, index=False)\n",
    "print(f\"边文件已保存至：{edges_path}\")"
   ],
   "id": "667b1fb693b397ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "边文件已保存至：/Users/chenenqiang/Desktop/Undergraduate Life/Undergraduate Life/创新实验2025春/FrequentPatternMiningBasedOnHotspotTrajectories/DataPreprocess/outputs/edges.csv\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T11:35:12.450039Z",
     "start_time": "2025-04-16T11:35:12.421581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "uri = \"bolt://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "password = \"#020728Ceq\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "# 清空整个数据库（慎用）\n",
    "with driver.session() as session:\n",
    "    session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "\n",
    "print(\"Neo4j 已清空所有节点和关系。\")\n",
    "\n",
    "driver.close()"
   ],
   "id": "22dd117618272df4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neo4j 已清空所有节点和关系。\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T11:36:22.335891Z",
     "start_time": "2025-04-16T11:36:15.424888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 路径配置\n",
    "base_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "root_dir = os.path.abspath(os.path.join(base_dir, '..'))\n",
    "output_dir = os.path.join(root_dir, 'outputs')\n",
    "nodes_path = os.path.join(output_dir, 'nodes.csv')\n",
    "edges_path = os.path.join(output_dir, 'edges.csv')\n",
    "\n",
    "# Neo4j 连接信息\n",
    "uri = \"bolt://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "password = \"#020728Ceq\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "# 节点导入函数\n",
    "def import_node(tx, node_id, x, y):\n",
    "    tx.run(\"\"\"\n",
    "        MERGE (n:Hotspot {id: $node_id})\n",
    "        SET n.x = $x, n.y = $y\n",
    "    \"\"\", node_id=node_id, x=x, y=y)\n",
    "\n",
    "# 边导入函数\n",
    "def import_edge(tx, source, target, frequency, traj_ids_flat):\n",
    "    tx.run(\"\"\"\n",
    "        MATCH (a:Hotspot {id: $source})\n",
    "        MATCH (b:Hotspot {id: $target})\n",
    "        MERGE (a)-[r:TRAJ_EDGE]->(b)\n",
    "        SET r.frequency = $frequency,\n",
    "            r.traj_ids = $traj_ids\n",
    "    \"\"\", source=source, target=target, frequency=frequency, traj_ids=traj_ids_flat)\n",
    "\n",
    "with driver.session() as session:\n",
    "    print(\"导入节点中...\")\n",
    "    nodes_df = pd.read_csv(nodes_path)\n",
    "    for _, row in nodes_df.iterrows():\n",
    "        session.execute_write(import_node, int(row['node_id']), float(row['x']), float(row['y']))\n",
    "\n",
    "    print(\"导入边中...\")\n",
    "    edges_df = pd.read_csv(edges_path)\n",
    "    for _, row in edges_df.iterrows():\n",
    "        # 修复嵌套数组问题：将 [[1,2],[2,3]] → [\"1_2\", \"2_3\"]\n",
    "        raw_traj_ids = json.loads(row['traj_ids']) if isinstance(row['traj_ids'], str) else []\n",
    "        traj_ids_flat = [f\"{uid}_{tid}\" for uid, tid in raw_traj_ids]\n",
    "\n",
    "        session.execute_write(\n",
    "            import_edge,\n",
    "            int(row['source']),\n",
    "            int(row['target']),\n",
    "            int(row['frequency']),\n",
    "            traj_ids_flat\n",
    "        )\n",
    "\n",
    "driver.close()\n",
    "print(\"Neo4j 数据导入完成\")"
   ],
   "id": "a06854ac1d79eb18",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "导入节点中...\n",
      "导入边中...\n",
      "Neo4j 数据导入完成\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T12:53:26.456968Z",
     "start_time": "2025-04-16T12:53:26.444597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ================= 工具函数 =================\n",
    "def safe_parse_json_list(x):\n",
    "    if isinstance(x, str):\n",
    "        return json.loads(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def preprocess_ndegree_df(df):\n",
    "    df['SN'] = df['SN'].apply(safe_parse_json_list)\n",
    "    df['SG'] = df['SG'].apply(lambda x: set(tuple(i) for i in safe_parse_json_list(x)))\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_result(df, filename, output_dir='outputs'):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # 强制转换 path 为 JSON 字符串格式，防止 tuple 存储出错\n",
    "    if 'path' in df.columns:\n",
    "        df['path'] = df['path'].apply(lambda x: json.dumps(list(x)) if isinstance(x, (list, tuple)) else x)\n",
    "\n",
    "    out_path = os.path.join(output_dir, filename)\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"结果已保存到: {out_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# ================= NDTTJ 算法 =================\n",
    "def run_ndttj(ndegree_df, m=5, k=2, save_as=None):\n",
    "    ndegree_df = preprocess_ndegree_df(ndegree_df)\n",
    "    ndttj_result = []\n",
    "    path_table = defaultdict(list)\n",
    "    for row in ndegree_df.itertuples():\n",
    "        if len(row.SG) >= m:\n",
    "            path_table[row.k].append((tuple(row.SN), row.SG))\n",
    "\n",
    "    final_result = {}\n",
    "    for ki in sorted(path_table.keys()):\n",
    "        for p1, sg1 in path_table[ki]:\n",
    "            for p2, sg2 in path_table[ki]:\n",
    "                if p1[1:] == p2[:-1]:\n",
    "                    new_path = p1 + (p2[-1],)\n",
    "                    if new_path in final_result:\n",
    "                        continue\n",
    "                    new_sg = sg1 & sg2\n",
    "                    if len(new_sg) >= m and len(new_path) >= k:\n",
    "                        final_result[new_path] = new_sg\n",
    "\n",
    "    for path, sg in final_result.items():\n",
    "        ndttj_result.append({\n",
    "            'path': path,\n",
    "            'frequency': len(sg),\n",
    "            'traj_ids': json.dumps(sorted(list(sg)), ensure_ascii=False)\n",
    "        })\n",
    "\n",
    "    df_result = pd.DataFrame(ndttj_result)\n",
    "    if save_as:\n",
    "        save_result(df_result, save_as)\n",
    "    return df_result\n",
    "\n",
    "\n",
    "# ================= NDTTT 算法 =================\n",
    "def run_ndttt(ndegree_df, m=5, k=2, save_as=None):\n",
    "    ndegree_df = preprocess_ndegree_df(ndegree_df)\n",
    "    ndttt_result = []\n",
    "    path_dict = {tuple(row.SN): row.SG for row in ndegree_df.itertuples() if len(row.SG) >= m}\n",
    "    visited = set()\n",
    "\n",
    "    def extend(path, sg):\n",
    "        results = []\n",
    "        for p in path_dict:\n",
    "            if len(p) == len(path) + 1 and p[:-1] == path:\n",
    "                new_sg = sg & path_dict[p]\n",
    "                if len(new_sg) >= m and len(p) >= k and p not in visited:\n",
    "                    visited.add(p)\n",
    "                    results.append((p, new_sg))\n",
    "                    results += extend(p, new_sg)\n",
    "        return results\n",
    "\n",
    "    for path, sg in path_dict.items():\n",
    "        if len(path) >= k:\n",
    "            results = extend(path, sg)\n",
    "            for p, sg_p in results:\n",
    "                ndttt_result.append({\n",
    "                    'path': p,\n",
    "                    'frequency': len(sg_p),\n",
    "                    'traj_ids': json.dumps(sorted(list(sg_p)), ensure_ascii=False)\n",
    "                })\n",
    "\n",
    "    df_result = pd.DataFrame(ndttt_result)\n",
    "    if save_as:\n",
    "        save_result(df_result, save_as)\n",
    "    return df_result\n",
    "\n",
    "\n",
    "# ================= TTHS 算法 =================\n",
    "def run_tths_from_neo4j(uri, user, password, m=5, k=2, save_as=None):\n",
    "    driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "    results = []\n",
    "    visited_paths = set()\n",
    "\n",
    "    def dfs(tx, path, traj_ids):\n",
    "        if len(path) >= k and len(traj_ids) >= m:\n",
    "            key = tuple(path)\n",
    "            if key not in visited_paths:\n",
    "                visited_paths.add(key)\n",
    "                results.append({\n",
    "                    'path': path[:],\n",
    "                    'frequency': len(traj_ids),\n",
    "                    'traj_ids': json.dumps(sorted(list(traj_ids)), ensure_ascii=False)\n",
    "                })\n",
    "        if len(path) > 12:\n",
    "            return\n",
    "\n",
    "        query = \"\"\"\n",
    "        MATCH (n:Hotspot {id: $nid})-[r:TRAJ_EDGE]->(m)\n",
    "        RETURN m.id AS next_id, r.traj_ids AS tids\n",
    "        \"\"\"\n",
    "        result = tx.run(query, nid=path[-1])\n",
    "        for record in result:\n",
    "            next_id = record['next_id']\n",
    "            if next_id in path:\n",
    "                continue\n",
    "            tids = set(tuple(map(int, tid.split('_'))) for tid in record['tids'])\n",
    "            intersected = traj_ids & tids\n",
    "            if len(intersected) >= m:\n",
    "                dfs(tx, path + [next_id], intersected)\n",
    "\n",
    "    with driver.session() as session:\n",
    "        start_nodes = session.run(\"MATCH (n:Hotspot) RETURN n.id AS nid\")\n",
    "        for record in start_nodes:\n",
    "            nid = record['nid']\n",
    "            edges = session.run(\"\"\"\n",
    "                MATCH (n:Hotspot {id: $nid})-[r:TRAJ_EDGE]->(m)\n",
    "                RETURN m.id AS next_id, r.traj_ids AS tids\n",
    "            \"\"\", nid=nid)\n",
    "            for edge in edges:\n",
    "                next_id = edge['next_id']\n",
    "                tids = set(tuple(map(int, tid.split('_'))) for tid in edge['tids'])\n",
    "                if len(tids) >= m:\n",
    "                    dfs(session, [nid, next_id], tids)\n",
    "\n",
    "    driver.close()\n",
    "    df_result = pd.DataFrame(results)\n",
    "    if save_as:\n",
    "        save_result(df_result, save_as)\n",
    "    return df_result"
   ],
   "id": "4528b9e59595c6f6",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T12:53:32.766124Z",
     "start_time": "2025-04-16T12:53:32.367196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ndegree_path_file = os.path.join(output_dir, \"ndegree_path_table.csv\")\n",
    "ndegree_df = pd.read_csv(ndegree_path_file)\n",
    "\n",
    "# 执行三个算法\n",
    "ndttj_df = run_ndttj(ndegree_df, m=5, k=2, save_as=\"ndttj_hotspot_paths.csv\")\n",
    "ndttt_df = run_ndttt(ndegree_df, m=5, k=2, save_as=\"ndttt_hotspot_paths.csv\")\n",
    "tths_df  = run_tths_from_neo4j(\n",
    "    uri=\"bolt://localhost:7687\",\n",
    "    user=\"neo4j\",\n",
    "    password=\"#020728Ceq\",\n",
    "    m=5, k=2,\n",
    "    save_as=\"tths_hotspot_paths.csv\"\n",
    ")\n",
    "\n",
    "print(\"NDTTJ 示例结果：\")\n",
    "print(ndttj_df.head())\n",
    "\n",
    "print(\"NDTTT 示例结果：\")\n",
    "print(ndttt_df.head())\n",
    "\n",
    "print(\"TTHS  示例结果：\")\n",
    "print(tths_df.head())"
   ],
   "id": "e5e03f1bb6c12c58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "结果已保存到: outputs/ndttj_hotspot_paths.csv\n",
      "结果已保存到: outputs/ndttt_hotspot_paths.csv\n",
      "结果已保存到: outputs/tths_hotspot_paths.csv\n",
      "NDTTJ 示例结果：\n",
      "          path  frequency                                           traj_ids\n",
      "0  [0, 1, 116]          6  [[35, 9], [35, 13], [35, 23], [35, 24], [35, 3...\n",
      "1    [1, 2, 1]          6  [[1, 16], [1, 35], [1, 36], [1, 62], [1, 64], ...\n",
      "2    [3, 0, 3]          7  [[1, 13], [5, 18], [5, 27], [5, 71], [96, 12],...\n",
      "3    [2, 1, 2]          6  [[1, 16], [1, 35], [1, 36], [1, 62], [1, 64], ...\n",
      "4    [2, 1, 0]          5      [[1, 28], [1, 31], [1, 59], [1, 63], [1, 69]]\n",
      "NDTTT 示例结果：\n",
      "          path  frequency                                           traj_ids\n",
      "0  [0, 1, 116]          6  [[35, 9], [35, 13], [35, 23], [35, 24], [35, 3...\n",
      "1    [2, 1, 0]          5      [[1, 28], [1, 31], [1, 59], [1, 63], [1, 69]]\n",
      "2   [2, 1, 23]          7  [[1, 35], [1, 36], [1, 39], [1, 45], [1, 62], ...\n",
      "3   [0, 34, 0]          6  [[5, 6], [96, 1], [96, 12], [96, 49], [179, 25...\n",
      "4   [0, 33, 0]          6  [[5, 11], [5, 22], [5, 38], [5, 52], [5, 53], ...\n",
      "TTHS  示例结果：\n",
      "       path  frequency                                           traj_ids\n",
      "0   [0, 41]          5    [[5, 50], [44, 12], [52, 9], [73, 9], [134, 8]]\n",
      "1  [0, 214]          7  [[96, 59], [96, 84], [179, 1], [179, 8], [179,...\n",
      "2  [0, 209]          8  [[91, 16], [91, 18], [91, 23], [91, 34], [91, ...\n",
      "3  [0, 205]          8  [[67, 93], [82, 41], [82, 46], [82, 50], [82, ...\n",
      "4  [0, 193]          5  [[67, 32], [82, 19], [82, 59], [82, 81], [82, ...\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T12:55:50.113567Z",
     "start_time": "2025-04-16T12:55:50.076606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "def safe_eval_traj_ids(x):\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    try:\n",
    "        return literal_eval(x)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "# === 安全解析 path ===\n",
    "def safe_parse_path(x):\n",
    "    if pd.isna(x):\n",
    "        return ()\n",
    "    if isinstance(x, list):\n",
    "        return tuple(x)\n",
    "    if isinstance(x, str) and x.strip().startswith('['):\n",
    "        try:\n",
    "            return tuple(json.loads(x))\n",
    "        except Exception:\n",
    "            return ()\n",
    "    return ()\n",
    "\n",
    "# === 加载数据并添加 source 标签 ===\n",
    "def load_with_source(path, source_name):\n",
    "    df = pd.read_csv(path)\n",
    "    df['path'] = df['path'].apply(safe_parse_path)\n",
    "    df['traj_ids'] = df['traj_ids'].apply(safe_eval_traj_ids)\n",
    "    df['source'] = [[source_name]] * len(df)\n",
    "    df = df[df['path'].apply(lambda x: len(x) > 0)]  # ✅ 去除空路径\n",
    "    return df\n",
    "\n",
    "ndttj_df = load_with_source(os.path.join(output_dir, 'ndttj_hotspot_paths.csv'), 'NDTTJ')\n",
    "ndttt_df = load_with_source(os.path.join(output_dir, 'ndttt_hotspot_paths.csv'), 'NDTTT')\n",
    "tths_df  = load_with_source(os.path.join(output_dir, 'tths_hotspot_paths.csv'),  'TTHS')\n",
    "\n",
    "# 合并前统计\n",
    "print(\"NDTTJ:\", len(ndttj_df), \"NDTTT:\", len(ndttt_df), \"TTHS:\", len(tths_df))\n",
    "\n",
    "# 合并\n",
    "merged_df = pd.concat([ndttj_df, ndttt_df, tths_df], ignore_index=True)\n",
    "\n",
    "def merge_groups(group):\n",
    "    merged_traj_ids = {tuple(x) for sublist in group['traj_ids'] for x in sublist}\n",
    "    merged_sources = sorted(set(src for sources in group['source'] for src in sources))\n",
    "    return pd.Series({\n",
    "        'frequency': max(group['frequency']),\n",
    "        'traj_ids': json.dumps(sorted(merged_traj_ids)),\n",
    "        'source': merged_sources\n",
    "    })\n",
    "\n",
    "merged_df = merged_df.groupby('path', group_keys=False).apply(merge_groups).reset_index()\n",
    "merged_df['path'] = merged_df['path'].apply(list)  # 输出为 JSON\n",
    "\n",
    "print(\"路径来源统计：\")\n",
    "print(merged_df['source'].explode().value_counts())\n",
    "output_path = os.path.join(output_dir, 'merged_hotspot_paths.csv')\n",
    "merged_df.to_csv(output_path, index=False)\n",
    "print(f\"合并完成，输出文件已保存：{output_path}\")"
   ],
   "id": "8fa092b430ca9515",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDTTJ: 65 NDTTT: 45 TTHS: 166\n",
      "路径来源统计：\n",
      "source\n",
      "TTHS     166\n",
      "NDTTJ     65\n",
      "NDTTT     45\n",
      "Name: count, dtype: int64\n",
      "合并完成，输出文件已保存：/Users/chenenqiang/Desktop/Undergraduate Life/Undergraduate Life/创新实验2025春/FrequentPatternMiningBasedOnHotspotTrajectories/DataPreprocess/outputs/merged_hotspot_paths.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/66/j4x4v1zx3w501lq05cqk8g8h0000gn/T/ipykernel_3258/3015770336.py:55: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  merged_df = merged_df.groupby('path', group_keys=False).apply(merge_groups).reset_index()\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T06:58:05.913884Z",
     "start_time": "2025-04-17T06:58:05.816804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 路径级时间特征增强模块\n",
    "# 基于 merged_hotspot_paths.csv 中的 traj_ids（[[uid, traj_id], ...]）\n",
    "# 与 traj_metadata.csv 中的 uid, traj_id, start_time 匹配，提取路径时间特征\n",
    "# 输出文件：merged_hotspot_paths_with_time.csv\n",
    "\n",
    "from collections import Counter\n",
    "from scipy.stats import entropy\n",
    "\n",
    "merged_path_file = os.path.join(output_dir, 'merged_hotspot_paths.csv')\n",
    "traj_metadata_file = os.path.join(output_dir, 'traj_metadata.csv')\n",
    "\n",
    "merged_df = pd.read_csv(merged_path_file)\n",
    "traj_meta_df = pd.read_csv(traj_metadata_file)\n",
    "\n",
    "merged_df['traj_ids'] = merged_df['traj_ids'].apply(json.loads)\n",
    "merged_df['path'] = merged_df['path'].apply(json.loads)\n",
    "\n",
    "traj_meta_df['traj_key'] = traj_meta_df.apply(lambda row: [int(row['uid']), int(row['traj_id'])], axis=1)\n",
    "traj_meta_df['start_hour'] = pd.to_datetime(traj_meta_df['start_time']).dt.hour + pd.to_datetime(traj_meta_df['start_time']).dt.minute / 60\n",
    "traj_dict = dict(zip(traj_meta_df['traj_key'].map(tuple), traj_meta_df['start_hour']))\n",
    "\n",
    "def get_time_entropy(hours):\n",
    "    binned = [int(h) for h in hours]  # 整小时分箱\n",
    "    freq = Counter(binned)\n",
    "    prob = np.array(list(freq.values())) / sum(freq.values())\n",
    "    return round(entropy(prob, base=2), 3) if len(prob) > 1 else 0.0\n",
    "\n",
    "def get_peak_period(hours):\n",
    "    bins = {\n",
    "        'morning_peak': range(6, 10),\n",
    "        'midday': range(10, 14),\n",
    "        'afternoon': range(14, 17),\n",
    "        'evening_peak': range(17, 21),\n",
    "        'night': list(range(21, 24)) + list(range(0, 6))\n",
    "    }\n",
    "    hour_counts = Counter([int(h) for h in hours])\n",
    "    period_counts = {k: sum(hour_counts[h] for h in v) for k, v in bins.items()}\n",
    "    return max(period_counts, key=period_counts.get)\n",
    "\n",
    "time_features = []\n",
    "\n",
    "for row in merged_df.itertuples():\n",
    "    traj_keys = [tuple(tid) for tid in row.traj_ids if tuple(tid) in traj_dict]\n",
    "    start_hours = [traj_dict[k] for k in traj_keys if k in traj_dict]\n",
    "\n",
    "    if start_hours:\n",
    "        avg_hour = round(np.mean(start_hours), 2)\n",
    "        time_ent = get_time_entropy(start_hours)\n",
    "        peak_period = get_peak_period(start_hours)\n",
    "    else:\n",
    "        avg_hour = np.nan\n",
    "        time_ent = np.nan\n",
    "        peak_period = None\n",
    "\n",
    "    time_features.append({\n",
    "        'path': row.path,\n",
    "        'avg_start_hour': avg_hour,\n",
    "        'time_entropy': time_ent,\n",
    "        'peak_period': peak_period\n",
    "    })\n",
    "\n",
    "time_df = pd.DataFrame(time_features)\n",
    "final_df = pd.merge(merged_df, time_df, on='path', how='left')\n",
    "out_path = os.path.join(output_dir, 'merged_hotspot_paths_with_time.csv')\n",
    "final_df.to_csv(out_path, index=False)\n",
    "print(f\"路径时间特征增强完成，输出文件：{out_path}\")"
   ],
   "id": "9d08d6ba7715267",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "路径时间特征增强完成，输出文件：/Users/chenenqiang/Desktop/Undergraduate Life/Undergraduate Life/创新实验2025春/FrequentPatternMiningBasedOnHotspotTrajectories/DataPreprocess/outputs/merged_hotspot_paths_with_time.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/66/j4x4v1zx3w501lq05cqk8g8h0000gn/T/ipykernel_1592/2237441411.py:21: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  traj_meta_df['start_hour'] = pd.to_datetime(traj_meta_df['start_time']).dt.hour + pd.to_datetime(traj_meta_df['start_time']).dt.minute / 60\n",
      "/var/folders/66/j4x4v1zx3w501lq05cqk8g8h0000gn/T/ipykernel_1592/2237441411.py:21: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  traj_meta_df['start_hour'] = pd.to_datetime(traj_meta_df['start_time']).dt.hour + pd.to_datetime(traj_meta_df['start_time']).dt.minute / 60\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T07:22:24.726290Z",
     "start_time": "2025-04-17T07:22:24.593555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 路径级空间特征增强模块\n",
    "# 基于 merged_hotspot_paths.csv + traj_metadata.csv + nodes.csv\n",
    "# 输出文件：merged_hotspot_paths_with_time_space.csv\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "merged_path_file = os.path.join(output_dir, 'merged_hotspot_paths.csv')\n",
    "traj_metadata_file = os.path.join(output_dir, 'traj_metadata.csv')\n",
    "nodes_file = os.path.join(output_dir, 'nodes.csv')\n",
    "merged_df = pd.read_csv(merged_path_file)\n",
    "traj_meta_df = pd.read_csv(traj_metadata_file)\n",
    "nodes_df = pd.read_csv(nodes_file)\n",
    "\n",
    "merged_df['traj_ids'] = merged_df['traj_ids'].apply(json.loads)\n",
    "merged_df['path'] = merged_df['path'].apply(json.loads)\n",
    "\n",
    "traj_meta_df['traj_key'] = traj_meta_df.apply(lambda row: [int(row['uid']), int(row['traj_id'])], axis=1)\n",
    "traj_meta_df['start_hour'] = pd.to_datetime(traj_meta_df['start_time']).dt.hour + pd.to_datetime(traj_meta_df['start_time']).dt.minute / 60\n",
    "traj_dict = dict(zip(traj_meta_df['traj_key'].map(tuple), traj_meta_df['start_hour']))\n",
    "\n",
    "# 构建 node_id -> (x, y) 映射\n",
    "coord_map = dict(zip(nodes_df['node_id'], zip(nodes_df['x'], nodes_df['y'])))\n",
    "\n",
    "def get_time_entropy(hours):\n",
    "    binned = [int(h) for h in hours]\n",
    "    freq = Counter(binned)\n",
    "    prob = np.array(list(freq.values())) / sum(freq.values())\n",
    "    return round(entropy(prob, base=2), 3) if len(prob) > 1 else 0.0\n",
    "\n",
    "def get_peak_period(hours):\n",
    "    bins = {\n",
    "        'morning_peak': range(6, 10),\n",
    "        'midday': range(10, 14),\n",
    "        'afternoon': range(14, 17),\n",
    "        'evening_peak': range(17, 21),\n",
    "        'night': list(range(21, 24)) + list(range(0, 6))\n",
    "    }\n",
    "    hour_counts = Counter([int(h) for h in hours])\n",
    "    period_counts = {k: sum(hour_counts[h] for h in v) for k, v in bins.items()}\n",
    "    return max(period_counts, key=period_counts.get)\n",
    "\n",
    "def calc_distance(p1, p2):\n",
    "    return sqrt((p1[0]-p2[0])**2 + (p1[1]-p2[1])**2)\n",
    "\n",
    "def calc_spatial_entropy(coords):\n",
    "    if len(coords) < 2:\n",
    "        return 0.0\n",
    "    xs = [round(x, 3) for x, y in coords]\n",
    "    ys = [round(y, 3) for x, y in coords]\n",
    "    px = Counter(xs)\n",
    "    py = Counter(ys)\n",
    "    prob_x = np.array(list(px.values())) / sum(px.values())\n",
    "    prob_y = np.array(list(py.values())) / sum(py.values())\n",
    "    return round(entropy(prob_x, base=2) + entropy(prob_y, base=2), 3)\n",
    "\n",
    "# 特征提取\n",
    "feature_list = []\n",
    "\n",
    "for row in merged_df.itertuples():\n",
    "    # 时间部分\n",
    "    traj_keys = [tuple(tid) for tid in row.traj_ids if tuple(tid) in traj_dict]\n",
    "    start_hours = [traj_dict[k] for k in traj_keys if k in traj_dict]\n",
    "    if start_hours:\n",
    "        avg_hour = round(np.mean(start_hours), 2)\n",
    "        time_ent = get_time_entropy(start_hours)\n",
    "        peak_period = get_peak_period(start_hours)\n",
    "    else:\n",
    "        avg_hour = np.nan\n",
    "        time_ent = np.nan\n",
    "        peak_period = None\n",
    "\n",
    "    # 空间部分\n",
    "    path_coords = [coord_map.get(nid) for nid in row.path if nid in coord_map]\n",
    "    if len(path_coords) >= 2:\n",
    "        spatial_len = sum(calc_distance(path_coords[i], path_coords[i+1]) for i in range(len(path_coords)-1))\n",
    "        center_x = round(np.mean([pt[0] for pt in path_coords]), 6)\n",
    "        center_y = round(np.mean([pt[1] for pt in path_coords]), 6)\n",
    "        spatial_ent = calc_spatial_entropy(path_coords)\n",
    "    else:\n",
    "        spatial_len = np.nan\n",
    "        center_x = np.nan\n",
    "        center_y = np.nan\n",
    "        spatial_ent = np.nan\n",
    "\n",
    "    feature_list.append({\n",
    "        'path': row.path,\n",
    "        'avg_start_hour': avg_hour,\n",
    "        'time_entropy': time_ent,\n",
    "        'peak_period': peak_period,\n",
    "        'path_length': len(row.path),\n",
    "        'spatial_length': round(spatial_len, 3) if not np.isnan(spatial_len) else np.nan,\n",
    "        'center_x': center_x,\n",
    "        'center_y': center_y,\n",
    "        'spatial_entropy': spatial_ent\n",
    "    })\n",
    "\n",
    "# ==== 合并输出 ====\n",
    "feature_df = pd.DataFrame(feature_list)\n",
    "final_df = pd.merge(merged_df, feature_df, on='path', how='left')\n",
    "out_path = os.path.join(output_dir, 'merged_hotspot_paths_with_time_space.csv')\n",
    "final_df.to_csv(out_path, index=False)\n",
    "print(f\"路径时间+空间特征增强完成，输出文件：{out_path}\")\n"
   ],
   "id": "dd67e797382759dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 路径时间+空间特征增强完成，输出文件：/Users/chenenqiang/Desktop/Undergraduate Life/Undergraduate Life/创新实验2025春/FrequentPatternMiningBasedOnHotspotTrajectories/DataPreprocess/outputs/merged_hotspot_paths_with_time_space.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/66/j4x4v1zx3w501lq05cqk8g8h0000gn/T/ipykernel_1592/3203226846.py:22: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  traj_meta_df['start_hour'] = pd.to_datetime(traj_meta_df['start_time']).dt.hour + pd.to_datetime(traj_meta_df['start_time']).dt.minute / 60\n",
      "/var/folders/66/j4x4v1zx3w501lq05cqk8g8h0000gn/T/ipykernel_1592/3203226846.py:22: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  traj_meta_df['start_hour'] = pd.to_datetime(traj_meta_df['start_time']).dt.hour + pd.to_datetime(traj_meta_df['start_time']).dt.minute / 60\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T07:53:59.549024Z",
     "start_time": "2025-04-17T07:50:54.050985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 节点级 POI 信息提取（高德地图 API）\n",
    "# 功能：读取 nodes.csv（含 node_id, x, y），调用逆地理编码 API\n",
    "# 输出 nodes_with_poi.csv，含 POI 名称、类型、地址\n",
    "\n",
    "AMAP_API_KEY = '' # 自行申请\n",
    "INPUT_FILE = os.path.join('..', 'outputs', 'nodes.csv')\n",
    "OUTPUT_FILE = os.path.join('..', 'outputs', 'nodes_with_poi.csv')\n",
    "CACHE_FILE = os.path.join('..', 'outputs', 'poi_cache.csv')\n",
    "SLEEP_INTERVAL = 0.5\n",
    "\n",
    "if os.path.exists(CACHE_FILE):\n",
    "    poi_cache = pd.read_csv(CACHE_FILE).set_index('node_id').to_dict('index')\n",
    "else:\n",
    "    poi_cache = {}\n",
    "\n",
    "nodes_df = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "# API 请求函数\n",
    "def query_poi(x, y):\n",
    "    url = f\"https://restapi.amap.com/v3/geocode/regeo\"\n",
    "    params = {\n",
    "        'location': f\"{x},{y}\",\n",
    "        'key': AMAP_API_KEY,\n",
    "        'output': 'json',\n",
    "        'radius': 100,\n",
    "        'extensions': 'all'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=5)\n",
    "        data = response.json()\n",
    "        if 'regeocode' in data and 'pois' in data['regeocode'] and len(data['regeocode']['pois']) > 0:\n",
    "            poi = data['regeocode']['pois'][0]  # 取第一个最相关的POI\n",
    "            return poi.get('name', None), poi.get('type', None), poi.get('address', None)\n",
    "        else:\n",
    "            return None, None, None\n",
    "    except Exception as e:\n",
    "        print(f\"请求失败: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# 遍历节点并提取 POI\n",
    "results = []\n",
    "\n",
    "for row in nodes_df.itertuples():\n",
    "    nid = row.node_id\n",
    "    x, y = row.x, row.y\n",
    "\n",
    "    if nid in poi_cache:\n",
    "        info = poi_cache[nid]\n",
    "    else:\n",
    "        name, typ, addr = query_poi(x, y)\n",
    "        info = {'poi_name': name, 'poi_type': typ, 'poi_address': addr}\n",
    "        poi_cache[nid] = info\n",
    "        time.sleep(SLEEP_INTERVAL)\n",
    "\n",
    "    results.append({\n",
    "        'node_id': nid,\n",
    "        'x': x,\n",
    "        'y': y,\n",
    "        'poi_name': info['poi_name'],\n",
    "        'poi_type': info['poi_type'],\n",
    "        'poi_address': info['poi_address']\n",
    "    })\n",
    "\n",
    "poi_df = pd.DataFrame(results)\n",
    "poi_df.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"POI提取完成，共保存 {len(poi_df)} 条节点信息：{OUTPUT_FILE}\")\n",
    "\n",
    "cache_df = pd.DataFrame.from_dict(poi_cache, orient='index')\n",
    "cache_df.index.name = 'node_id'\n",
    "cache_df.reset_index().to_csv(CACHE_FILE, index=False)\n",
    "print(f\"缓存已更新：{CACHE_FILE}\")\n"
   ],
   "id": "c4e436ca43b2983d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POI提取完成，共保存 247 条节点信息：../outputs/nodes_with_poi.csv\n",
      "缓存已更新：../outputs/poi_cache.csv\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T08:00:05.249958Z",
     "start_time": "2025-04-17T08:00:05.213193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 路径级时间 + 空间 + 语义（POI）特征增强模块\n",
    "# 输入：\n",
    "# - merged_hotspot_paths_with_time_space.csv\n",
    "# - nodes_with_poi.csv（含 node_id, poi_type, ...）\n",
    "# 输出：\n",
    "# - merged_hotspot_paths_with_time_space_semantic.csv\n",
    "\n",
    "from collections import Counter\n",
    "from scipy.stats import entropy\n",
    "\n",
    "input_path = os.path.join(output_dir, 'merged_hotspot_paths_with_time_space.csv')\n",
    "poi_file = os.path.join(output_dir, 'nodes_with_poi.csv')\n",
    "df = pd.read_csv(input_path)\n",
    "poi_df = pd.read_csv(poi_file)\n",
    "\n",
    "df['path'] = df['path'].apply(json.loads)\n",
    "\n",
    "# 构建 node_id -> poi_type（一级）映射\n",
    "def extract_main_category(poi_type_str):\n",
    "    if pd.isna(poi_type_str):\n",
    "        return None\n",
    "    parts = poi_type_str.split(';')\n",
    "    return parts[0] if parts else None\n",
    "\n",
    "poi_df['main_poi'] = poi_df['poi_type'].apply(extract_main_category)\n",
    "poi_map = dict(zip(poi_df['node_id'], poi_df['main_poi']))\n",
    "\n",
    "# 辅助函数\n",
    "def calc_entropy(poi_list):\n",
    "    count = Counter(poi_list)\n",
    "    prob = np.array(list(count.values())) / sum(count.values())\n",
    "    return round(entropy(prob, base=2), 3) if len(prob) > 1 else 0.0\n",
    "\n",
    "# 特征提取\n",
    "semantic_features = []\n",
    "\n",
    "for row in df.itertuples():\n",
    "    node_ids = row.path\n",
    "    pois = [poi_map[n] for n in node_ids if n in poi_map and poi_map[n] is not None]\n",
    "\n",
    "    if pois:\n",
    "        poi_types = sorted(set(pois))\n",
    "        dominant_poi = Counter(pois).most_common(1)[0][0]\n",
    "        poi_ent = calc_entropy(pois)\n",
    "    else:\n",
    "        poi_types = []\n",
    "        dominant_poi = None\n",
    "        poi_ent = np.nan\n",
    "\n",
    "    semantic_features.append({\n",
    "        'path': row.path,\n",
    "        'poi_types': poi_types,\n",
    "        'dominant_poi': dominant_poi,\n",
    "        'poi_entropy': poi_ent\n",
    "    })\n",
    "\n",
    "# 合并特征\n",
    "semantic_df = pd.DataFrame(semantic_features)\n",
    "final_df = pd.merge(df, semantic_df, on='path', how='left')\n",
    "out_path = os.path.join(output_dir, 'merged_hotspot_paths_with_time_space_semantic.csv')\n",
    "final_df.to_csv(out_path, index=False)\n",
    "print(f\"POI语义增强完成，输出文件：{out_path}\")\n"
   ],
   "id": "36048402f63955b9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POI语义增强完成，输出文件：/Users/chenenqiang/Desktop/Undergraduate Life/Undergraduate Life/创新实验2025春/FrequentPatternMiningBasedOnHotspotTrajectories/DataPreprocess/outputs/merged_hotspot_paths_with_time_space_semantic.csv\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T09:13:51.493944Z",
     "start_time": "2025-04-17T09:13:51.478229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. 过滤 path_length == 1 （单节点路径）\n",
    "# 2. 填充缺失 dominant_poi / poi_types\n",
    "# 3. IQR 修剪 spatial_length 极端值\n",
    "\n",
    "SRC_FILE   = os.path.join(output_dir, 'merged_hotspot_paths_with_time_space_semantic.csv')\n",
    "DST_FILE   = os.path.join(output_dir, 'cleaned_paths.csv')\n",
    "\n",
    "df = pd.read_csv(SRC_FILE)\n",
    "\n",
    "df = df[df['path_length'] > 1].reset_index(drop=True)\n",
    "\n",
    "df['dominant_poi'] = df['dominant_poi'].fillna('无')\n",
    "# poi_types 本身是字符串化列表；缺失时置为 []\n",
    "df['poi_types'] = df['poi_types'].fillna('[]')\n",
    "\n",
    "q1 = df['spatial_length'].quantile(0.25)\n",
    "q3 = df['spatial_length'].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "lower_bound = max(0, q1 - 1.5 * iqr)\n",
    "\n",
    "df = df[(df['spatial_length'] >= lower_bound) & (df['spatial_length'] <= upper_bound)].reset_index(drop=True)\n",
    "\n",
    "df.to_csv(DST_FILE, index=False)\n",
    "print(f\"高级清洗完成，保存到: {DST_FILE}\")\n",
    "print(f\"剩余路径数: {len(df)}\")"
   ],
   "id": "efd14b887eaddd75",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "高级清洗完成，保存到: /Users/chenenqiang/Desktop/Undergraduate Life/Undergraduate Life/创新实验2025春/FrequentPatternMiningBasedOnHotspotTrajectories/DataPreprocess/outputs/cleaned_paths.csv\n",
      "剩余路径数: 194\n"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
