{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T07:40:16.022628Z",
     "start_time": "2025-04-24T07:40:16.018808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 基础模块\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "# 可视化配置\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['font.family'] = 'STHeiti'         # 中文支持（macOS）\n",
    "plt.rcParams['axes.unicode_minus'] = False      # 正负号支持\n",
    "%matplotlib inline\n",
    "\n",
    "# 轨迹聚类与建模\n",
    "from sklearn.cluster import DBSCAN\n",
    "from pyproj import Transformer\n",
    "\n",
    "# API调用（如POI增强）\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Neo4j 图数据库\n",
    "from py2neo import Graph, Node, Relationship  # 若报错先注释，等后面阶段再装\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# 路径配置\n",
    "base_dir = os.path.dirname(os.path.abspath(\"__file__\"))  # 当前脚本所在目录\n",
    "root_dir = os.path.abspath(os.path.join(base_dir, '..'))  # 项目根目录\n",
    "output_dir = os.path.join(root_dir, 'outputs')\n",
    "traj_path = os.path.join(output_dir, 'geolife_cleaned_traj.csv')"
   ],
   "id": "c1c197d2efc8e319",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-16T11:14:22.869455Z",
     "start_time": "2025-04-16T11:14:22.687798Z"
    }
   },
   "source": [
    "# Step 1：加载数据\n",
    "df = pd.read_csv(traj_path)\n",
    "df['t'] = pd.to_datetime(df['t'])\n",
    "\n",
    "# Step 2：空间聚类识别热点节点\n",
    "eps = 0.0006      # 空间阈值（近似50米）\n",
    "min_samples = 5\n",
    "db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "df['node'] = db.fit_predict(df[['x', 'y']])\n",
    "\n",
    "# 移除噪声节点\n",
    "df = df[df['node'] != -1].reset_index(drop=True)\n",
    "\n",
    "# Step 3：构建轨迹节点序列 & 时间序列\n",
    "traj_records = []\n",
    "\n",
    "for (uid, traj_id), group in df.groupby(['uid', 'traj_id']):\n",
    "    group = group.sort_values('t')\n",
    "    nodes = group['node'].tolist()\n",
    "    times = group['t'].tolist()\n",
    "\n",
    "    # 去除连续重复节点\n",
    "    clean_nodes = [nodes[0]]\n",
    "    clean_times = [times[0]]\n",
    "    for i in range(1, len(nodes)):\n",
    "        if nodes[i] != clean_nodes[-1]:\n",
    "            clean_nodes.append(nodes[i])\n",
    "            clean_times.append(times[i])\n",
    "\n",
    "    if len(clean_nodes) >= 2:\n",
    "        traj_records.append({\n",
    "            'uid': int(uid),\n",
    "            'traj_id': int(traj_id),\n",
    "            'start_time': clean_times[0].time(),\n",
    "            'end_time': clean_times[-1].time(),\n",
    "            'node_sequence': json.dumps(clean_nodes, ensure_ascii=False),\n",
    "            'time_sequence': json.dumps([t.strftime(\"%H:%M:%S\") for t in clean_times], ensure_ascii=False)\n",
    "        })\n",
    "\n",
    "print(f\"构建完成，共记录轨迹数：{len(traj_records)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "构建完成，共记录轨迹数：1181\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T11:15:34.135038Z",
     "start_time": "2025-04-16T11:15:34.125557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "traj_meta_df = pd.DataFrame(traj_records)\n",
    "metadata_path = os.path.join(output_dir, 'traj_metadata.csv')\n",
    "traj_meta_df.to_csv(metadata_path, index=False)\n",
    "\n",
    "print(f\"轨迹元数据表已保存：{metadata_path}\")"
   ],
   "id": "95ae1913359dcf98",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "轨迹元数据表已保存：/Users/chenenqiang/Desktop/Undergraduate Life/Undergraduate Life/创新实验2025春/FrequentPatternMiningBasedOnHotspotTrajectories/DataPreprocess/outputs/traj_metadata.csv\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T08:19:09.162097Z",
     "start_time": "2025-04-23T08:19:09.121296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "root_dir = os.path.abspath(os.path.join(base_dir, '..'))\n",
    "output_dir = os.path.join(root_dir, 'outputs')\n",
    "meta_path = os.path.join(output_dir, 'traj_metadata.csv')\n",
    "\n",
    "# 加载轨迹元数据\n",
    "df = pd.read_csv(meta_path)\n",
    "df['node_sequence'] = df['node_sequence'].apply(json.loads)\n",
    "\n",
    "# 构建1阶路径表：(start_node, end_node) → 轨迹集合（uid-traj_id）\n",
    "one_degree_path_table = defaultdict(set)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    uid = int(row['uid'])\n",
    "    traj_id = int(row['traj_id'])\n",
    "    nodes = row['node_sequence']\n",
    "\n",
    "    for i in range(len(nodes) - 1):\n",
    "        path = (nodes[i], nodes[i + 1])\n",
    "        one_degree_path_table[path].add(f\"{uid}-{traj_id}\")\n",
    "\n",
    "print(f\"1阶路径构建完成，共有唯一路径：{len(one_degree_path_table)} 条\")\n",
    "\n",
    "# 输出为CSV标准结构：start_node, end_node, traj_set\n",
    "one_degree_path_df = pd.DataFrame([\n",
    "    {\n",
    "        'start_node': path[0],\n",
    "        'end_node': path[1],\n",
    "        'traj_set': json.dumps(sorted(list(traj_set)), ensure_ascii=False)\n",
    "    }\n",
    "    for path, traj_set in one_degree_path_table.items()\n",
    "])\n",
    "\n",
    "# 保存1阶路径表\n",
    "output_file = os.path.join(output_dir, 'one_degree_path_table.csv')\n",
    "one_degree_path_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"1阶路径表已保存至：{output_file}\")"
   ],
   "id": "10d8a58c68849b89",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1阶路径构建完成，共有唯一路径：1152 条\n",
      "1阶路径表已保存至：/Users/chenenqiang/Desktop/Undergraduate Life/Undergraduate Life/创新实验2025春/FrequentPatternMiningBasedOnHotspotTrajectories/DataPreprocess/outputs/one_degree_path_table.csv\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T11:26:33.425553Z",
     "start_time": "2025-04-16T11:26:33.362764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 加载原始轨迹数据\n",
    "df = pd.read_csv(traj_path)\n",
    "\n",
    "# DBSCAN聚类参数（保持与之前完全一致）\n",
    "eps = 0.0006\n",
    "min_samples = 5\n",
    "db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "df['node'] = db.fit_predict(df[['x', 'y']])\n",
    "df = df[df['node'] != -1]  # 移除噪声\n",
    "\n",
    "# 计算每个 node_id 的坐标中心\n",
    "node_coords = df.groupby('node')[['x', 'y']].mean().reset_index()\n",
    "node_coords.columns = ['node_id', 'x', 'y']\n",
    "\n",
    "# 保存为 nodes.csv\n",
    "nodes_path = os.path.join(output_dir, 'nodes.csv')\n",
    "node_coords.to_csv(nodes_path, index=False)\n",
    "print(f\"节点中心文件已保存至：{nodes_path}\")"
   ],
   "id": "907dc52d8c5f5f6c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "节点中心文件已保存至：/Users/chenenqiang/Desktop/Undergraduate Life/Undergraduate Life/创新实验2025春/FrequentPatternMiningBasedOnHotspotTrajectories/DataPreprocess/outputs/nodes.csv\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T11:27:21.882155Z",
     "start_time": "2025-04-16T11:27:21.850336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 读取 traj_metadata\n",
    "meta_path = os.path.join(output_dir, 'traj_metadata.csv')\n",
    "meta_df = pd.read_csv(meta_path)\n",
    "meta_df['node_sequence'] = meta_df['node_sequence'].apply(json.loads)\n",
    "\n",
    "# 构建边频率与所属轨迹集合\n",
    "edge_freq = defaultdict(int)\n",
    "edge_trajs = defaultdict(set)\n",
    "\n",
    "for _, row in meta_df.iterrows():\n",
    "    uid, traj_id = int(row['uid']), int(row['traj_id'])\n",
    "    nodes = row['node_sequence']\n",
    "\n",
    "    for i in range(len(nodes) - 1):\n",
    "        edge = (nodes[i], nodes[i+1])\n",
    "        edge_freq[edge] += 1\n",
    "        edge_trajs[edge].add((uid, traj_id))\n",
    "\n",
    "# 构建输出表\n",
    "edges_df = pd.DataFrame([\n",
    "    {\n",
    "        'source': src,\n",
    "        'target': tgt,\n",
    "        'frequency': edge_freq[(src, tgt)],\n",
    "        'traj_ids': json.dumps(sorted([list(x) for x in edge_trajs[(src, tgt)]]), ensure_ascii=False)\n",
    "    }\n",
    "    for (src, tgt) in edge_freq\n",
    "])\n",
    "\n",
    "# 保存为 edges.csv\n",
    "edges_path = os.path.join(output_dir, 'edges.csv')\n",
    "edges_df.to_csv(edges_path, index=False)\n",
    "print(f\"边文件已保存至：{edges_path}\")"
   ],
   "id": "667b1fb693b397ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "边文件已保存至：/Users/chenenqiang/Desktop/Undergraduate Life/Undergraduate Life/创新实验2025春/FrequentPatternMiningBasedOnHotspotTrajectories/DataPreprocess/outputs/edges.csv\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T11:35:12.450039Z",
     "start_time": "2025-04-16T11:35:12.421581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "uri = \"bolt://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "password = \"#020728Ceq\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "# 清空整个数据库（慎用）\n",
    "with driver.session() as session:\n",
    "    session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "\n",
    "print(\"Neo4j 已清空所有节点和关系。\")\n",
    "\n",
    "driver.close()"
   ],
   "id": "22dd117618272df4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neo4j 已清空所有节点和关系。\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T11:36:22.335891Z",
     "start_time": "2025-04-16T11:36:15.424888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 路径配置\n",
    "base_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "root_dir = os.path.abspath(os.path.join(base_dir, '..'))\n",
    "output_dir = os.path.join(root_dir, 'outputs')\n",
    "nodes_path = os.path.join(output_dir, 'nodes.csv')\n",
    "edges_path = os.path.join(output_dir, 'edges.csv')\n",
    "\n",
    "# Neo4j 连接信息\n",
    "uri = \"bolt://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "password = \"#020728Ceq\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "# 节点导入函数\n",
    "def import_node(tx, node_id, x, y):\n",
    "    tx.run(\"\"\"\n",
    "        MERGE (n:Hotspot {id: $node_id})\n",
    "        SET n.x = $x, n.y = $y\n",
    "    \"\"\", node_id=node_id, x=x, y=y)\n",
    "\n",
    "# 边导入函数\n",
    "def import_edge(tx, source, target, frequency, traj_ids_flat):\n",
    "    tx.run(\"\"\"\n",
    "        MATCH (a:Hotspot {id: $source})\n",
    "        MATCH (b:Hotspot {id: $target})\n",
    "        MERGE (a)-[r:TRAJ_EDGE]->(b)\n",
    "        SET r.frequency = $frequency,\n",
    "            r.traj_ids = $traj_ids\n",
    "    \"\"\", source=source, target=target, frequency=frequency, traj_ids=traj_ids_flat)\n",
    "\n",
    "with driver.session() as session:\n",
    "    print(\"导入节点中...\")\n",
    "    nodes_df = pd.read_csv(nodes_path)\n",
    "    for _, row in nodes_df.iterrows():\n",
    "        session.execute_write(import_node, int(row['node_id']), float(row['x']), float(row['y']))\n",
    "\n",
    "    print(\"导入边中...\")\n",
    "    edges_df = pd.read_csv(edges_path)\n",
    "    for _, row in edges_df.iterrows():\n",
    "        # 修复嵌套数组问题：将 [[1,2],[2,3]] → [\"1_2\", \"2_3\"]\n",
    "        raw_traj_ids = json.loads(row['traj_ids']) if isinstance(row['traj_ids'], str) else []\n",
    "        traj_ids_flat = [f\"{uid}_{tid}\" for uid, tid in raw_traj_ids]\n",
    "\n",
    "        session.execute_write(\n",
    "            import_edge,\n",
    "            int(row['source']),\n",
    "            int(row['target']),\n",
    "            int(row['frequency']),\n",
    "            traj_ids_flat\n",
    "        )\n",
    "\n",
    "driver.close()\n",
    "print(\"Neo4j 数据导入完成\")"
   ],
   "id": "a06854ac1d79eb18",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "导入节点中...\n",
      "导入边中...\n",
      "Neo4j 数据导入完成\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T07:57:41.731796Z",
     "start_time": "2025-04-24T07:57:41.720376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------- 通用工具 ----------\n",
    "def safe_parse_json_list(x):\n",
    "    return json.loads(x) if isinstance(x, str) else x\n",
    "\n",
    "def preprocess_onedegree_df(df, start_col=\"start_node\", end_col=\"end_node\", traj_col=\"traj_set\"):\n",
    "    \"\"\"\n",
    "    将轨迹集合列转为 set，并统一列名结构。\n",
    "    \"\"\"\n",
    "    rename_map = {start_col: \"start_node\", end_col: \"end_node\", traj_col: \"traj_set\"}\n",
    "    df = df.rename(columns=rename_map)[[\"start_node\", \"end_node\", \"traj_set\"]].copy()\n",
    "    df[\"traj_set\"] = df[\"traj_set\"].apply(lambda x: set(safe_parse_json_list(x)))\n",
    "    return df\n",
    "\n",
    "def save_result(df, filename, output_dir=None):\n",
    "    if output_dir is None:\n",
    "        current_dir = os.getcwd()\n",
    "        output_dir = os.path.join(current_dir, \"..\", \"outputs\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    if \"path\" in df.columns:\n",
    "        df[\"path\"] = df[\"path\"].apply(lambda p: json.dumps(list(p)) if isinstance(p, (list, tuple)) else p)\n",
    "    out = os.path.join(output_dir, filename)\n",
    "    df.to_csv(out, index=False)\n",
    "    print(f\"结果已保存到: {out}\")\n",
    "\n",
    "# ---------- NDTTJ ----------\n",
    "def run_ndttj(one_degree_df, m, k,\n",
    "              save_as=None, max_depth=None):\n",
    "    \"\"\"\n",
    "    NDTTJ：基于 N 度路径表连接的 Apriori-Join 热点挖掘\n",
    "    ------------------\n",
    "    one_degree_df : 1 阶路径表 DataFrame\n",
    "    m             : 频繁度阈值 (support)\n",
    "    k             : 最小路径长度\n",
    "    max_depth     : 最长允许的节点数（含首尾）。None 表示不限制\n",
    "    \"\"\"\n",
    "    df = preprocess_onedegree_df(one_degree_df)\n",
    "    edge_dict = {(r.start_node, r.end_node): r.traj_set\n",
    "                 for r in df.itertuples()}\n",
    "\n",
    "    result = {}\n",
    "    # 初始队列：所有满足支持度的一阶边\n",
    "    queue = deque([((u, v), s)\n",
    "                   for (u, v), s in edge_dict.items()\n",
    "                   if len(s) >= m])\n",
    "\n",
    "    while queue:\n",
    "        path, sg = queue.popleft()\n",
    "\n",
    "        # 写结果\n",
    "        if len(path) >= k:\n",
    "            result[path] = sg\n",
    "\n",
    "        # 如果已达最大深度就停止向下拼接\n",
    "        if max_depth is not None and len(path) >= max_depth:\n",
    "            continue\n",
    "\n",
    "        tail = path[-1]\n",
    "        # 用尾节点拼接下一条边\n",
    "        for (x, y), s2 in edge_dict.items():\n",
    "            if x != tail:\n",
    "                continue\n",
    "            new_sg = sg & s2\n",
    "            if len(new_sg) < m:\n",
    "                continue\n",
    "            new_path = path + (y,)\n",
    "            # 只有第一次出现时才入队\n",
    "            if new_path not in result:\n",
    "                queue.append((new_path, new_sg))\n",
    "\n",
    "    out_df = pd.DataFrame([{\n",
    "            \"path\": p,\n",
    "            \"frequency\": len(s),\n",
    "            \"traj_ids\": json.dumps(sorted(s))\n",
    "        } for p, s in result.items()])\n",
    "\n",
    "    if save_as:\n",
    "        save_result(out_df, save_as)\n",
    "    return out_df\n",
    "\n",
    "# ---------- NDTTT ----------\n",
    "def run_ndttt(one_degree_df, m, k, save_as=None, max_depth=12):\n",
    "    df = preprocess_onedegree_df(one_degree_df)\n",
    "    edge_dict = defaultdict(list)\n",
    "    for r in df.itertuples():\n",
    "        if len(r.traj_set) >= m:\n",
    "            edge_dict[r.start_node].append((r.end_node, r.traj_set))\n",
    "\n",
    "    result = {}\n",
    "    # 显式栈元素: (path_tuple, traj_set)\n",
    "    for u, lst in edge_dict.items():\n",
    "        for v, sg in lst:\n",
    "            stack = [((u, v), sg)]\n",
    "            while stack:\n",
    "                path, cur_sg = stack.pop()\n",
    "                if len(path) >= k:\n",
    "                    result[path] = cur_sg\n",
    "                if len(path) >= max_depth:\n",
    "                    continue\n",
    "                tail = path[-1]\n",
    "                for nxt, sg2 in edge_dict.get(tail, []):\n",
    "                    new_sg = cur_sg & sg2\n",
    "                    if len(new_sg) < m:\n",
    "                        continue\n",
    "                    new_path = path + (nxt,)\n",
    "                    if new_path not in result:\n",
    "                        stack.append((new_path, new_sg))\n",
    "\n",
    "    out_df = pd.DataFrame(\n",
    "        [{\"path\": p, \"frequency\": len(s), \"traj_ids\": json.dumps(sorted(s))} for p, s in result.items()]\n",
    "    )\n",
    "    if save_as:\n",
    "        save_result(out_df, save_as)\n",
    "    return out_df\n",
    "\n",
    "# ---------- TTHS ----------\n",
    "def run_tths_from_neo4j(uri, user, password, m, k, save_as=None):\n",
    "    driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "    results = []\n",
    "    visited_paths = set()\n",
    "\n",
    "    def dfs(tx, path, traj_ids):\n",
    "        if len(path) >= k and len(traj_ids) >= m:\n",
    "            key = tuple(path)\n",
    "            if key not in visited_paths:\n",
    "                visited_paths.add(key)\n",
    "                results.append({\n",
    "                    'path': path[:],\n",
    "                    'frequency': len(traj_ids),\n",
    "                    'traj_ids': json.dumps(sorted(list(traj_ids)), ensure_ascii=False)\n",
    "                })\n",
    "        if len(path) > 12:\n",
    "            return\n",
    "\n",
    "        query = \"\"\"\n",
    "        MATCH (n:Hotspot {id: $nid})-[r:TRAJ_EDGE]->(m)\n",
    "        RETURN m.id AS next_id, r.traj_ids AS tids\n",
    "        \"\"\"\n",
    "        result = tx.run(query, nid=path[-1])\n",
    "        for record in result:\n",
    "            next_id = record['next_id']\n",
    "            if next_id in path:\n",
    "                continue\n",
    "            tids = set(tuple(map(int, tid.split('_'))) for tid in record['tids'])\n",
    "            intersected = traj_ids & tids\n",
    "            if len(intersected) >= m:\n",
    "                dfs(tx, path + [next_id], intersected)\n",
    "\n",
    "    with driver.session() as session:\n",
    "        start_nodes = session.run(\"MATCH (n:Hotspot) RETURN n.id AS nid\")\n",
    "        for record in start_nodes:\n",
    "            nid = record['nid']\n",
    "            edges = session.run(\"\"\"\n",
    "                MATCH (n:Hotspot {id: $nid})-[r:TRAJ_EDGE]->(m)\n",
    "                RETURN m.id AS next_id, r.traj_ids AS tids\n",
    "            \"\"\", nid=nid)\n",
    "            for edge in edges:\n",
    "                next_id = edge['next_id']\n",
    "                tids = set(tuple(map(int, tid.split('_'))) for tid in edge['tids'])\n",
    "                if len(tids) >= m:\n",
    "                    dfs(session, [nid, next_id], tids)\n",
    "\n",
    "    driver.close()\n",
    "    df_result = pd.DataFrame(results)\n",
    "    if save_as:\n",
    "        save_result(df_result, save_as)\n",
    "    return df_result"
   ],
   "id": "4528b9e59595c6f6",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T07:57:47.135149Z",
     "start_time": "2025-04-24T07:57:46.705833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 调用三个算法\n",
    "base_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "root_dir = os.path.abspath(os.path.join(base_dir, '..'))\n",
    "output_dir = os.path.join(root_dir, 'outputs')\n",
    "one_degree_path_file = os.path.join(output_dir, \"one_degree_path_table.csv\")\n",
    "one_degree_df = pd.read_csv(one_degree_path_file)\n",
    "\n",
    "ndttj_df = run_ndttj(one_degree_df, m=4, k=3, save_as=\"ndttj_hotspot_paths.csv\", max_depth=8)\n",
    "ndttt_df = run_ndttt(one_degree_df, m=4, k=3, save_as=\"ndttt_hotspot_paths.csv\", max_depth=12)\n",
    "tths_df = run_tths_from_neo4j(\n",
    "    uri=\"bolt://localhost:7687\",\n",
    "    user=\"neo4j\",\n",
    "    password=\"#020728Ceq\",\n",
    "    m=4, k=3,\n",
    "    save_as=\"tths_hotspot_paths.csv\"\n",
    ")\n",
    "\n",
    "print(\"NDTTJ 示例结果：\")\n",
    "print(ndttj_df.head())\n",
    "\n",
    "print(\"NDTTT 示例结果：\")\n",
    "print(ndttt_df.head())\n",
    "\n",
    "print(\"TTHS  示例结果：\")\n",
    "print(tths_df.head())"
   ],
   "id": "e5e03f1bb6c12c58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "结果已保存到: /Users/chenenqiang/Desktop/Undergraduate Life/Undergraduate Life/创新实验2025春/FrequentPatternMiningBasedOnHotspotTrajectories/DataPreprocess/scripts/../outputs/ndttj_hotspot_paths.csv\n",
      "结果已保存到: /Users/chenenqiang/Desktop/Undergraduate Life/Undergraduate Life/创新实验2025春/FrequentPatternMiningBasedOnHotspotTrajectories/DataPreprocess/scripts/../outputs/ndttt_hotspot_paths.csv\n",
      "结果已保存到: /Users/chenenqiang/Desktop/Undergraduate Life/Undergraduate Life/创新实验2025春/FrequentPatternMiningBasedOnHotspotTrajectories/DataPreprocess/scripts/../outputs/tths_hotspot_paths.csv\n",
      "NDTTJ 示例结果：\n",
      "          path  frequency                                           traj_ids\n",
      "0  [0, 1, 116]          6  [\"35-13\", \"35-23\", \"35-24\", \"35-38\", \"35-42\", ...\n",
      "1    [1, 2, 1]          6   [\"1-16\", \"1-35\", \"1-36\", \"1-62\", \"1-64\", \"1-69\"]\n",
      "2    [3, 0, 3]          7  [\"1-13\", \"179-21\", \"179-27\", \"5-18\", \"5-27\", \"...\n",
      "3    [2, 1, 2]          6   [\"1-16\", \"1-35\", \"1-36\", \"1-62\", \"1-64\", \"1-69\"]\n",
      "4    [2, 1, 0]          5           [\"1-28\", \"1-31\", \"1-59\", \"1-63\", \"1-69\"]\n",
      "NDTTT 示例结果：\n",
      "                 path  frequency  \\\n",
      "0         [0, 1, 116]          6   \n",
      "1           [0, 3, 0]          7   \n",
      "2        [0, 3, 0, 3]          7   \n",
      "3     [0, 3, 0, 3, 0]          7   \n",
      "4  [0, 3, 0, 3, 0, 3]          7   \n",
      "\n",
      "                                            traj_ids  \n",
      "0  [\"35-13\", \"35-23\", \"35-24\", \"35-38\", \"35-42\", ...  \n",
      "1  [\"1-13\", \"179-21\", \"179-27\", \"5-18\", \"5-27\", \"...  \n",
      "2  [\"1-13\", \"179-21\", \"179-27\", \"5-18\", \"5-27\", \"...  \n",
      "3  [\"1-13\", \"179-21\", \"179-27\", \"5-18\", \"5-27\", \"...  \n",
      "4  [\"1-13\", \"179-21\", \"179-27\", \"5-18\", \"5-27\", \"...  \n",
      "TTHS  示例结果：\n",
      "               path  frequency  \\\n",
      "0     [0, 164, 214]          8   \n",
      "1     [0, 164, 209]          4   \n",
      "2     [0, 116, 115]          4   \n",
      "3      [0, 115, 40]         15   \n",
      "4  [0, 115, 40, 83]          6   \n",
      "\n",
      "                                            traj_ids  \n",
      "0  [[96, 13], [96, 92], [96, 103], [179, 32], [17...  \n",
      "1           [[91, 15], [91, 47], [91, 72], [91, 80]]  \n",
      "2           [[35, 28], [35, 29], [35, 57], [35, 70]]  \n",
      "3  [[35, 20], [35, 22], [35, 28], [35, 30], [35, ...  \n",
      "4  [[35, 20], [35, 28], [35, 35], [35, 37], [35, ...  \n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T08:14:25.638163Z",
     "start_time": "2025-04-24T08:14:25.523396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "# 安全解析 traj_ids，TTHS 的格式特殊需转换\n",
    "def safe_eval_traj_ids(x, convert_uid_traj=False):\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    if isinstance(x, list):\n",
    "        result = x\n",
    "    else:\n",
    "        try:\n",
    "            result = literal_eval(x)\n",
    "        except Exception:\n",
    "            return []\n",
    "    if convert_uid_traj:\n",
    "        # TTHS 专用：把 [[uid, tid], ...] 转换成 ['uid-tid', ...]\n",
    "        return [f\"{uid}-{tid}\" for uid, tid in result if isinstance(uid, int) and isinstance(tid, int)]\n",
    "    return result\n",
    "\n",
    "def safe_parse_path(x):\n",
    "    if pd.isna(x):\n",
    "        return ()\n",
    "    if isinstance(x, list):\n",
    "        return tuple(x)\n",
    "    if isinstance(x, str) and x.strip().startswith('['):\n",
    "        try:\n",
    "            return tuple(json.loads(x))\n",
    "        except Exception:\n",
    "            return ()\n",
    "    return ()\n",
    "\n",
    "# 加载并标记来源，TTHS 要多传一个参数\n",
    "def load_with_source(path, source_name, convert_uid_traj=False):\n",
    "    df = pd.read_csv(path)\n",
    "    df['path'] = df['path'].apply(safe_parse_path)\n",
    "    df['traj_ids'] = df['traj_ids'].apply(lambda x: safe_eval_traj_ids(x, convert_uid_traj=convert_uid_traj))\n",
    "    df['source'] = [[source_name]] * len(df)\n",
    "    df = df[df['path'].apply(lambda x: isinstance(x, tuple) and len(x) > 0)]\n",
    "    return df\n",
    "\n",
    "# 设置输出目录（与 scripts 平行的 outputs 文件夹）\n",
    "output_dir = \"../outputs\"\n",
    "ndttj_df = load_with_source(os.path.join(output_dir, 'ndttj_hotspot_paths.csv'), 'NDTTJ')\n",
    "ndttt_df = load_with_source(os.path.join(output_dir, 'ndttt_hotspot_paths.csv'), 'NDTTT')\n",
    "# 注意这里传 convert_uid_traj=True 来处理嵌套格式\n",
    "tths_df  = load_with_source(os.path.join(output_dir, 'tths_hotspot_paths.csv'), 'TTHS', convert_uid_traj=True)\n",
    "\n",
    "# 合并前统计\n",
    "print(\"NDTTJ:\", len(ndttj_df), \"NDTTT:\", len(ndttt_df), \"TTHS:\", len(tths_df))\n",
    "\n",
    "# 合并数据\n",
    "merged_df = pd.concat([ndttj_df, ndttt_df, tths_df], ignore_index=True)\n",
    "\n",
    "# 合并规则：同 path 合并 traj_ids 与 source，保留最大 frequency\n",
    "def merge_groups(group):\n",
    "    merged_traj_ids = {tid for sublist in group['traj_ids'] for tid in sublist}\n",
    "    merged_sources = sorted(set(src for sources in group['source'] for src in sources))\n",
    "    return pd.Series({\n",
    "        'frequency': max(group['frequency']),\n",
    "        'traj_ids': json.dumps(sorted(merged_traj_ids)),\n",
    "        'source': merged_sources\n",
    "    })\n",
    "\n",
    "# 分组合并\n",
    "merged_df = merged_df.groupby('path', group_keys=False).apply(merge_groups).reset_index()\n",
    "\n",
    "# 输出格式转为 JSON 样式（仅最后转换）\n",
    "merged_df['path'] = merged_df['path'].apply(list)\n",
    "\n",
    "# 打印来源统计\n",
    "print(\"路径来源统计：\")\n",
    "print(merged_df['source'].explode().value_counts())\n",
    "\n",
    "# 保存合并结果\n",
    "output_path = os.path.join(output_dir, 'merged_hotspot_paths.csv')\n",
    "merged_df.to_csv(output_path, index=False)\n",
    "print(f\"合并完成，输出文件已保存：{output_path}\")"
   ],
   "id": "8fa092b430ca9515",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDTTJ: 478 NDTTT: 1134 TTHS: 58\n",
      "路径来源统计：\n",
      "source\n",
      "NDTTT    1134\n",
      "NDTTJ     478\n",
      "TTHS       58\n",
      "Name: count, dtype: int64\n",
      "合并完成，输出文件已保存：../outputs/merged_hotspot_paths.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/66/j4x4v1zx3w501lq05cqk8g8h0000gn/T/ipykernel_1622/1768534043.py:64: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  merged_df = merged_df.groupby('path', group_keys=False).apply(merge_groups).reset_index()\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T09:05:46.011400Z",
     "start_time": "2025-04-24T09:05:45.657379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#  路径级时间 & 空间特征增强\n",
    "#  ---------------------------------------------------------------\n",
    "#  - 输入 : merged_hotspot_paths.csv, traj_metadata.csv, nodes.csv\n",
    "#  - 输出 : merged_hotspot_paths_with_time_space.csv\n",
    "\n",
    "import os, json, ast, csv, warnings\n",
    "from math import sqrt\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ------------ 通用路径 ------------\n",
    "OUTPUT_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"outputs\"))\n",
    "\n",
    "MERGED_FILE = os.path.join(OUTPUT_DIR, \"merged_hotspot_paths.csv\")\n",
    "TRAJ_META_FILE = os.path.join(OUTPUT_DIR, \"traj_metadata.csv\")\n",
    "NODES_FILE = os.path.join(OUTPUT_DIR, \"nodes.csv\")\n",
    "TARGET_FILE = os.path.join(OUTPUT_DIR, \"merged_hotspot_paths_with_time_space.csv\")\n",
    "\n",
    "# ------------ 1. 载入 3 个文件 ------------\n",
    "merged_df = pd.read_csv(MERGED_FILE)\n",
    "traj_meta_df = pd.read_csv(TRAJ_META_FILE)\n",
    "nodes_df = pd.read_csv(NODES_FILE)\n",
    "\n",
    "# ------------ 2. 通用解析函数 ------------\n",
    "def parse_maybe_list(val):\n",
    "    \"\"\"兼容 json / python 表达式 / 已经是 list 的三种情况\"\"\"\n",
    "    if isinstance(val, list):\n",
    "        return val\n",
    "    if isinstance(val, (float, int)) and np.isnan(val):\n",
    "        return []\n",
    "    if isinstance(val, str):\n",
    "        txt = val.strip()\n",
    "        for loader in (json.loads, ast.literal_eval):\n",
    "            try:\n",
    "                return loader(txt)\n",
    "            except Exception:\n",
    "                continue\n",
    "    return []\n",
    "\n",
    "for col in [\"path\", \"traj_ids\", \"source\"]:\n",
    "    if col in merged_df.columns:\n",
    "        merged_df[col] = merged_df[col].apply(parse_maybe_list)\n",
    "\n",
    "# ------------ 3. 轨迹元数据预处理 ------------\n",
    "traj_meta_df[\"start_time\"] = pd.to_datetime(\n",
    "    traj_meta_df[\"start_time\"], errors=\"coerce\"\n",
    ")\n",
    "traj_meta_df[\"start_hour\"] = (\n",
    "    traj_meta_df[\"start_time\"].dt.hour.fillna(-1)\n",
    "    + traj_meta_df[\"start_time\"].dt.minute.fillna(0) / 60\n",
    ")\n",
    "\n",
    "traj_meta_df[\"traj_key\"] = list(\n",
    "    zip(traj_meta_df[\"uid\"].astype(int), traj_meta_df[\"traj_id\"].astype(int))\n",
    ")\n",
    "traj_hour_dict = dict(zip(traj_meta_df[\"traj_key\"], traj_meta_df[\"start_hour\"]))\n",
    "# -------------------------------------------------------------\n",
    "# 反序列化 path / traj_ids / source\n",
    "# -------------------------------------------------------------\n",
    "for col in [\"path\", \"traj_ids\", \"source\"]:\n",
    "    if col in merged_df.columns:\n",
    "        merged_df[col] = merged_df[col].apply(parse_maybe_list)\n",
    "\n",
    "# 把 traj_ids 统一转换成 (uid, traj_id) 键\n",
    "def to_traj_key(item):\n",
    "    \"\"\"\n",
    "    支持三种格式:\n",
    "    1) \"35-13\"        → (35, 13)\n",
    "    2) [35, 13]       → (35, 13)\n",
    "    3) (35, 13)       → (35, 13)\n",
    "    解析失败返回 None\n",
    "    \"\"\"\n",
    "    if isinstance(item, (list, tuple)) and len(item) == 2:\n",
    "        try:\n",
    "            return (int(item[0]), int(item[1]))\n",
    "        except Exception:\n",
    "            return None\n",
    "    if isinstance(item, str) and \"-\" in item:\n",
    "        a, b = item.split(\"-\", 1)\n",
    "        try:\n",
    "            return (int(float(a)), int(float(b)))\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "merged_df[\"traj_ids\"] = merged_df[\"traj_ids\"].apply(\n",
    "    lambda lst: [k for k in (to_traj_key(x) for x in lst) if k]\n",
    ")\n",
    "\n",
    "# ------------ 4. node 坐标字典 ------------\n",
    "coord_map = dict(zip(nodes_df[\"node_id\"], zip(nodes_df[\"x\"], nodes_df[\"y\"])))\n",
    "\n",
    "# ------------ 5. 辅助函数 ------------\n",
    "def time_entropy(hours):\n",
    "    if len(hours) <= 1:\n",
    "        return 0.0\n",
    "    cnt = Counter(map(int, hours))\n",
    "    prob = np.array(list(cnt.values())) / sum(cnt.values())\n",
    "    return round(float(entropy(prob, base=2)), 3)\n",
    "\n",
    "def peak_period(hours):\n",
    "    bins = {\n",
    "        \"morning_peak\": range(6, 10),\n",
    "        \"midday\": range(10, 14),\n",
    "        \"afternoon\": range(14, 17),\n",
    "        \"evening_peak\": range(17, 21),\n",
    "        \"night\": list(range(21, 24)) + list(range(0, 6)),\n",
    "    }\n",
    "    h_cnt = Counter(map(int, hours))\n",
    "    agg = {k: sum(h_cnt[h] for h in v) for k, v in bins.items()}\n",
    "    return max(agg, key=agg.get) if agg else np.nan\n",
    "\n",
    "def euclid(p1, p2):\n",
    "    return sqrt((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2)\n",
    "\n",
    "def spatial_entropy(coords):\n",
    "    if len(coords) < 2:\n",
    "        return 0.0\n",
    "    xs = [round(x, 3) for x, _ in coords]\n",
    "    ys = [round(y, 3) for _, y in coords]\n",
    "    px, py = Counter(xs), Counter(ys)\n",
    "    probx = np.array(list(px.values())) / sum(px.values())\n",
    "    proby = np.array(list(py.values())) / sum(py.values())\n",
    "    return round(float(entropy(probx, base=2) + entropy(proby, base=2)), 3)\n",
    "\n",
    "# ------------ 6. 计算特征（向量化写入） ------------\n",
    "def calc_features(row):\n",
    "    # ---- 时间特征 ----\n",
    "    keys = [tuple(t) for t in row[\"traj_ids\"] if tuple(t) in traj_hour_dict]\n",
    "    hours = [traj_hour_dict[k] for k in keys]\n",
    "\n",
    "    avg_start_hour = round(float(np.mean(hours)), 2) if hours else np.nan\n",
    "    t_entropy = time_entropy(hours) if hours else np.nan\n",
    "    p_period = peak_period(hours) if hours else np.nan\n",
    "\n",
    "    # ---- 空间特征 ----\n",
    "    coords = [coord_map.get(n) for n in row[\"path\"] if n in coord_map]\n",
    "    coords = [c for c in coords if c is not None]\n",
    "\n",
    "    path_length = len(row[\"path\"])\n",
    "    if len(coords) >= 2:\n",
    "        spatial_len = round(\n",
    "            sum(euclid(coords[i], coords[i + 1]) for i in range(len(coords) - 1)), 3\n",
    "        )\n",
    "        cx, cy = np.mean([c[0] for c in coords]), np.mean([c[1] for c in coords])\n",
    "        s_entropy = spatial_entropy(coords)\n",
    "    else:\n",
    "        spatial_len, cx, cy, s_entropy = (np.nan,) * 4\n",
    "\n",
    "    return pd.Series(\n",
    "        [\n",
    "            avg_start_hour,\n",
    "            t_entropy,\n",
    "            p_period,\n",
    "            path_length,\n",
    "            spatial_len,\n",
    "            round(cx, 6) if not np.isnan(cx) else np.nan,\n",
    "            round(cy, 6) if not np.isnan(cy) else np.nan,\n",
    "            s_entropy,\n",
    "        ],\n",
    "        index=[\n",
    "            \"avg_start_hour\",\n",
    "            \"time_entropy\",\n",
    "            \"peak_period\",\n",
    "            \"path_length\",\n",
    "            \"spatial_length\",\n",
    "            \"center_x\",\n",
    "            \"center_y\",\n",
    "            \"spatial_entropy\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "merged_df = merged_df.join(merged_df.apply(calc_features, axis=1))\n",
    "\n",
    "# ------------ 7. 重新序列化 & 保存 ------------\n",
    "for col in [\"path\", \"traj_ids\", \"source\"]:\n",
    "    merged_df[col] = merged_df[col].apply(json.dumps)\n",
    "\n",
    "merged_df.to_csv(TARGET_FILE, index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "print(f\"路径时空特征已写入: {TARGET_FILE}\")"
   ],
   "id": "dd67e797382759dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "路径时空特征已写入: /Users/chenenqiang/Desktop/Undergraduate Life/Undergraduate Life/创新实验2025春/FrequentPatternMiningBasedOnHotspotTrajectories/DataPreprocess/outputs/merged_hotspot_paths_with_time_space.csv\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T07:53:59.549024Z",
     "start_time": "2025-04-17T07:50:54.050985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 节点级 POI 信息提取（高德地图 API）\n",
    "# 功能：读取 nodes.csv（含 node_id, x, y），调用逆地理编码 API\n",
    "# 输出 nodes_with_poi.csv，含 POI 名称、类型、地址\n",
    "\n",
    "AMAP_API_KEY = '' # 自行申请\n",
    "INPUT_FILE = os.path.join('..', 'outputs', 'nodes.csv')\n",
    "OUTPUT_FILE = os.path.join('..', 'outputs', 'nodes_with_poi.csv')\n",
    "CACHE_FILE = os.path.join('..', 'outputs', 'poi_cache.csv')\n",
    "SLEEP_INTERVAL = 0.5\n",
    "\n",
    "if os.path.exists(CACHE_FILE):\n",
    "    poi_cache = pd.read_csv(CACHE_FILE).set_index('node_id').to_dict('index')\n",
    "else:\n",
    "    poi_cache = {}\n",
    "\n",
    "nodes_df = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "# API 请求函数\n",
    "def query_poi(x, y):\n",
    "    url = f\"https://restapi.amap.com/v3/geocode/regeo\"\n",
    "    params = {\n",
    "        'location': f\"{x},{y}\",\n",
    "        'key': AMAP_API_KEY,\n",
    "        'output': 'json',\n",
    "        'radius': 100,\n",
    "        'extensions': 'all'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=5)\n",
    "        data = response.json()\n",
    "        if 'regeocode' in data and 'pois' in data['regeocode'] and len(data['regeocode']['pois']) > 0:\n",
    "            poi = data['regeocode']['pois'][0]  # 取第一个最相关的POI\n",
    "            return poi.get('name', None), poi.get('type', None), poi.get('address', None)\n",
    "        else:\n",
    "            return None, None, None\n",
    "    except Exception as e:\n",
    "        print(f\"请求失败: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# 遍历节点并提取 POI\n",
    "results = []\n",
    "\n",
    "for row in nodes_df.itertuples():\n",
    "    nid = row.node_id\n",
    "    x, y = row.x, row.y\n",
    "\n",
    "    if nid in poi_cache:\n",
    "        info = poi_cache[nid]\n",
    "    else:\n",
    "        name, typ, addr = query_poi(x, y)\n",
    "        info = {'poi_name': name, 'poi_type': typ, 'poi_address': addr}\n",
    "        poi_cache[nid] = info\n",
    "        time.sleep(SLEEP_INTERVAL)\n",
    "\n",
    "    results.append({\n",
    "        'node_id': nid,\n",
    "        'x': x,\n",
    "        'y': y,\n",
    "        'poi_name': info['poi_name'],\n",
    "        'poi_type': info['poi_type'],\n",
    "        'poi_address': info['poi_address']\n",
    "    })\n",
    "\n",
    "poi_df = pd.DataFrame(results)\n",
    "poi_df.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"POI提取完成，共保存 {len(poi_df)} 条节点信息：{OUTPUT_FILE}\")\n",
    "\n",
    "cache_df = pd.DataFrame.from_dict(poi_cache, orient='index')\n",
    "cache_df.index.name = 'node_id'\n",
    "cache_df.reset_index().to_csv(CACHE_FILE, index=False)\n",
    "print(f\"缓存已更新：{CACHE_FILE}\")\n"
   ],
   "id": "c4e436ca43b2983d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POI提取完成，共保存 247 条节点信息：../outputs/nodes_with_poi.csv\n",
      "缓存已更新：../outputs/poi_cache.csv\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T09:11:18.645797Z",
     "start_time": "2025-04-24T09:11:18.540844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 路径级时间 + 空间 + 语义（POI）特征增强模块\n",
    "# 输入：\n",
    "# - merged_hotspot_paths_with_time_space.csv\n",
    "# - nodes_with_poi.csv（含 node_id, poi_type, ...）\n",
    "# 输出：\n",
    "# - merged_hotspot_paths_with_time_space_semantic.csv\n",
    "\n",
    "import os, json, warnings, numpy as np, pandas as pd\n",
    "from collections import Counter\n",
    "from scipy.stats import entropy\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "CWD = os.getcwd()\n",
    "output_dir = os.path.abspath(\n",
    "    os.path.join(CWD, \"..\", \"outputs\")\n",
    ")\n",
    "\n",
    "FILE_TIME_SPACE = os.path.join(\n",
    "    output_dir, \"merged_hotspot_paths_with_time_space.csv\"\n",
    ")\n",
    "FILE_POI = os.path.join(output_dir, \"nodes_with_poi.csv\")\n",
    "\n",
    "# ---------- 读取 ----------\n",
    "df  = pd.read_csv(FILE_TIME_SPACE)\n",
    "poi = pd.read_csv(FILE_POI)\n",
    "\n",
    "# ---- 反序列化 path / traj_ids（确保是 list）----\n",
    "if \"path\" in df.columns and df[\"path\"].dtype == object:\n",
    "    df[\"path\"] = df[\"path\"].apply(json.loads)\n",
    "\n",
    "if \"traj_ids\" in df.columns and df[\"traj_ids\"].dtype == object:\n",
    "    df[\"traj_ids\"] = df[\"traj_ids\"].apply(json.loads)\n",
    "\n",
    "# ---------- 处理 POI 表 ----------\n",
    "# nodes_with_poi.csv 至少要有：\n",
    "#   node_id, poi_type\n",
    "# 若 poi_type 多级（例：美食;快餐;汉堡） → 只取第一级\n",
    "def pick_main_cat(poi_type):\n",
    "    if pd.isna(poi_type):\n",
    "        return None\n",
    "    return poi_type.split(\";\")[0]\n",
    "\n",
    "poi[\"main_poi\"] = poi[\"poi_type\"].apply(pick_main_cat)\n",
    "\n",
    "# node_id -> 一级 POI\n",
    "node2poi = dict(zip(poi[\"node_id\"], poi[\"main_poi\"]))\n",
    "\n",
    "# ---------- 辅助函数 ----------\n",
    "def poi_entropy(pois):\n",
    "    \"\"\"离散熵，衡量 POI 多样性\"\"\"\n",
    "    if not pois:\n",
    "        return np.nan\n",
    "    c = Counter(pois)\n",
    "    prob = np.array(list(c.values())) / prob.sum() if (prob := np.array(list(c.values()))) .sum() else prob\n",
    "    return round(float(entropy(prob, base=2)), 3) if len(prob) > 1 else 0.0\n",
    "\n",
    "# ---------- 遍历路径 & 计算 ----------\n",
    "sem_rows = []\n",
    "\n",
    "for row in df.itertuples():\n",
    "    pois = [node2poi[n] for n in row.path if n in node2poi and node2poi[n] is not None]\n",
    "    if pois:\n",
    "        sem_rows.append({\n",
    "            \"path\"        : json.dumps(row.path, ensure_ascii=False),\n",
    "            \"poi_types\"   : json.dumps(sorted(set(pois)), ensure_ascii=False),\n",
    "            \"dominant_poi\": Counter(pois).most_common(1)[0][0],\n",
    "            \"poi_entropy\" : poi_entropy(pois)\n",
    "        })\n",
    "    else:                          # 找不到任何 POI\n",
    "        sem_rows.append({\n",
    "            \"path\"        : json.dumps(row.path, ensure_ascii=False),\n",
    "            \"poi_types\"   : \"[]\",\n",
    "            \"dominant_poi\": None,\n",
    "            \"poi_entropy\" : np.nan\n",
    "        })\n",
    "\n",
    "sem_df = pd.DataFrame(sem_rows)\n",
    "\n",
    "# ---------- 合并 & 输出 ----------\n",
    "# 注意：df 里的 path 列目前是 list，需要转为 JSON 字符串才能 on='path' merge\n",
    "df[\"path_json\"]  = df[\"path\"].apply(lambda x: json.dumps(x, ensure_ascii=False))\n",
    "sem_df.rename(columns={\"path\": \"path_json\"}, inplace=True)\n",
    "\n",
    "merged_sem = pd.merge(df.drop(columns=[\"path\"]), sem_df, on=\"path_json\", how=\"left\")\n",
    "\n",
    "# 把 path_json 改回名为 path，保持和之前格式一致\n",
    "merged_sem.rename(columns={\"path_json\": \"path\"}, inplace=True)\n",
    "\n",
    "TARGET_FILE = os.path.join(output_dir, \"merged_hotspot_paths_with_time_space_semantic.csv\")\n",
    "merged_sem.to_csv(TARGET_FILE, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"语义特征已追加，文件保存到：{TARGET_FILE}\\n共 {len(merged_sem)} 条路径\")"
   ],
   "id": "36048402f63955b9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "语义特征已追加，文件保存到：/Users/chenenqiang/Desktop/Undergraduate Life/Undergraduate Life/创新实验2025春/FrequentPatternMiningBasedOnHotspotTrajectories/DataPreprocess/outputs/merged_hotspot_paths_with_time_space_semantic.csv\n",
      "共 1134 条路径\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T09:16:13.353376Z",
     "start_time": "2025-04-24T09:16:13.325589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, json, warnings, pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "OUTPUT_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"outputs\"))\n",
    "\n",
    "SRC_FILE = os.path.join(OUTPUT_DIR,\n",
    "                        \"merged_hotspot_paths_with_time_space_semantic.csv\")\n",
    "DST_FILE = os.path.join(OUTPUT_DIR, \"cleaned_paths.csv\")\n",
    "\n",
    "# ------------- 读取 -------------\n",
    "df = pd.read_csv(SRC_FILE)\n",
    "\n",
    "# ------------- 基础清洗 -------------\n",
    "# 1) 去掉空 path 或 path_len<=1\n",
    "df[\"path\"] = df[\"path\"].apply(lambda x: json.loads(x) if isinstance(x, str) else x)\n",
    "df[\"path_length\"] = df[\"path\"].apply(len)\n",
    "df = df[df[\"path_length\"] > 1].reset_index(drop=True)\n",
    "\n",
    "# 2) dominant_poi / poi_types 缺失填充\n",
    "df[\"dominant_poi\"] = df[\"dominant_poi\"].fillna(\"无\")\n",
    "\n",
    "# 保证 poi_types 为合法 JSON 字符串\n",
    "def safe_poi_types(x):\n",
    "    if pd.isna(x) or x == \"\" or x == \"[]\":\n",
    "        return \"[]\"\n",
    "    if isinstance(x, list):\n",
    "        return json.dumps(x, ensure_ascii=False)\n",
    "    try:\n",
    "        _ = json.loads(x)\n",
    "        return x\n",
    "    except Exception:\n",
    "        # 粗糙字符串，例如 \"[商场,住宅]\" → 转成真 JSON\n",
    "        items = [s.strip(\" '\\\"\") for s in x.strip(\"[]\").split(\",\") if s.strip()]\n",
    "        return json.dumps(items, ensure_ascii=False)\n",
    "\n",
    "df[\"poi_types\"] = df[\"poi_types\"].apply(safe_poi_types)\n",
    "\n",
    "# ------------- IQR 修剪 spatial_length -------------\n",
    "# 有些行 spatial_length 可能 NaN —— 先丢再算分位数\n",
    "df_nonan = df.dropna(subset=[\"spatial_length\"])\n",
    "q1, q3 = df_nonan[\"spatial_length\"].quantile([0.25, 0.75])\n",
    "iqr = q3 - q1\n",
    "upper = q3 + 1.5 * iqr\n",
    "lower = max(0, q1 - 1.5 * iqr)\n",
    "\n",
    "df = df[(df[\"spatial_length\"].isna()) |  # 保留无法计算长度的\n",
    "        ((df[\"spatial_length\"] >= lower) & (df[\"spatial_length\"] <= upper))] \\\n",
    "        .reset_index(drop=True)\n",
    "\n",
    "# ------------- 再序列化 JSON 列 -------------\n",
    "json_cols = [\"path\", \"traj_ids\", \"source\"]\n",
    "for col in json_cols:\n",
    "    df[col] = df[col].apply(lambda x: json.dumps(x, ensure_ascii=False)\n",
    "                            if not isinstance(x, str) else x)\n",
    "\n",
    "# ------------- 保存 -------------\n",
    "df.to_csv(DST_FILE, index=False, encoding=\"utf-8\")\n",
    "print(f\"高级清洗完成 -> {DST_FILE}\")\n",
    "print(f\"剩余路径数: {len(df)}\")"
   ],
   "id": "efd14b887eaddd75",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "高级清洗完成 -> /Users/chenenqiang/Desktop/Undergraduate Life/Undergraduate Life/创新实验2025春/FrequentPatternMiningBasedOnHotspotTrajectories/DataPreprocess/outputs/cleaned_paths.csv\n",
      "剩余路径数: 1106\n"
     ]
    }
   ],
   "execution_count": 32
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
